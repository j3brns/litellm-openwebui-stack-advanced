version: '3.8'

services:
  openwebui:
    image: ghcr.io/open-webui/open-webui:stable
    container_name: open-webui
    ports:
      - "3000:8080"
    volumes:
      - open-webui:/app/backend/data
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      - WEBUI_AUTH=${ENABLE_AUTH:-True}
      - OPENAI_API_KEY=${MASTER_KEY}
      - OPENAI_API_BASE_URL=http://litellm:4000/v1
      - NODE_ENV=${NODE_ENV:-production}
    restart: unless-stopped
    depends_on:
      litellm:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 512M
    networks:
      - llm-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  litellm:
    image: ghcr.io/berriai/litellm-database:0.11.0
    container_name: litellm
    env_file:
      - .env
    ports:
      - "4000:4000"
    volumes:
      - ./config.yml:/app/config.yml:ro
      - ./logs:/app/logs
    command: ["--config", "/app/config.yml", "--port", "4000", "${DEBUG_FLAG}"]
    restart: unless-stopped
    depends_on:
      db:
        condition: service_healthy
    environment:
      DATABASE_URL: "${DATABASE_URL:-postgresql://llmproxy:${DB_PASSWORD}@db:5432/litellm}"
      STORE_MODEL_IN_DB: "True"
      LOG_LEVEL: "${LOG_LEVEL:-info}"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1.5G
        reservations:
          cpus: '0.5'
          memory: 1G
    networks:
      - llm-network
    logging:
      driver: "json-file"
      options:
        max-size: "20m"
        max-file: "5"

  db:
    image: postgres:15-alpine
    container_name: litellm-db
    restart: unless-stopped
    environment:
      POSTGRES_DB: litellm
      POSTGRES_USER: llmproxy
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d litellm -U llmproxy"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 256M
    networks:
      - llm-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  prometheus:
    image: prom/prometheus:v2.45.0
    container_name: litellm-prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    ports:
      - "9090:9090"
    restart: unless-stopped
    depends_on:
      litellm:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
    networks:
      - llm-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  grafana:
    image: grafana/grafana:10.0.3
    container_name: litellm-grafana
    volumes:
      - grafana-data:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
    ports:
      - "3001:3000"
    restart: unless-stopped
    depends_on:
      - prometheus
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
    networks:
      - llm-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

volumes:
  open-webui:
    name: open-webui-data
  postgres-data:
    name: litellm-postgres-data
  prometheus-data:
    name: litellm-prometheus-data
  grafana-data:
    name: litellm-grafana-data

networks:
  llm-network:
    driver: bridge
